name: Daily Database Backup

on:
  schedule:
    # Tous les jours Ã  3h du matin (UTC) = 4h heure franÃ§aise
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      upload_to_gcs:
        description: 'Upload backup to Google Cloud Storage'
        type: boolean
        default: true

env:
  # Secrets from organization
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  # Variables from repository
  REGION: ${{ vars.GCP_REGION }}
  BACKUP_BUCKET: ${{ vars.BACKUP_BUCKET }}
  DATABASE_URL_SECRET: ${{ vars.DATABASE_URL_SECRET }}
  BACKUP_RETENTION_DAYS: ${{ vars.BACKUP_RETENTION_DAYS }}

jobs:
  backup:
    name: Backup Neon Database
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      id-token: write

    steps:
      - uses: actions/checkout@v4

      - name: Google Auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCS_GH_SVC_ACCOUNT_JSON_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y wget ca-certificates lsb-release
          
          # Add PostgreSQL repository for version 17 (modern keyring method)
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo gpg --dearmor -o /usr/share/keyrings/postgresql-keyring.gpg
          echo "deb [signed-by=/usr/share/keyrings/postgresql-keyring.gpg] http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main" | sudo tee /etc/apt/sources.list.d/pgdg.list
          
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17

      - name: Verify pg_dump version
        run: |
          pg_dump --version

      - name: Get Database URL from Secret Manager
        id: db-url
        run: |
          DB_URL=$(gcloud secrets versions access latest --secret=${{ env.DATABASE_URL_SECRET }})
          
          # Convert asyncpg URL to psql URL
          PSQL_URL=$(echo "$DB_URL" | sed 's/postgresql+asyncpg/postgresql/')
          
          # Extract components from URL to avoid query parameter issues
          # Format: postgresql://user:pass@host:port/dbname?params
          if [[ $PSQL_URL =~ postgresql://([^:]+):([^@]+)@([^:/]+):?([0-9]*)/([^\?]+)(\?.*)? ]]; then
            DB_USER="${BASH_REMATCH[1]}"
            DB_PASS="${BASH_REMATCH[2]}"
            DB_HOST="${BASH_REMATCH[3]}"
            DB_PORT="${BASH_REMATCH[4]:-5432}"
            DB_NAME="${BASH_REMATCH[5]}"
            
            echo "::add-mask::$DB_PASS"
            echo "user=$DB_USER" >> $GITHUB_OUTPUT
            echo "password=$DB_PASS" >> $GITHUB_OUTPUT
            echo "host=$DB_HOST" >> $GITHUB_OUTPUT
            echo "port=$DB_PORT" >> $GITHUB_OUTPUT
            echo "database=$DB_NAME" >> $GITHUB_OUTPUT
          else
            echo "âŒ Failed to parse DATABASE_URL"
            exit 1
          fi

      - name: Create Backup
        id: backup
        env:
          PGPASSWORD: ${{ steps.db-url.outputs.password }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="budget_backup_${TIMESTAMP}.sql"
          
          echo "Creating backup: $BACKUP_FILE"
          # Use individual parameters instead of URL to avoid SSL query param issues
          pg_dump \
            -h "${{ steps.db-url.outputs.host }}" \
            -p "${{ steps.db-url.outputs.port }}" \
            -U "${{ steps.db-url.outputs.user }}" \
            -d "${{ steps.db-url.outputs.database }}" \
            --no-password \
            > "$BACKUP_FILE"
          
          # Compress
          gzip "$BACKUP_FILE"
          BACKUP_FILE="${BACKUP_FILE}.gz"
          
          echo "file=$BACKUP_FILE" >> $GITHUB_OUTPUT
          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT
          
          # Show backup info
          ls -lh "$BACKUP_FILE"

      - name: Create GCS Bucket if not exists
        run: |
          RETENTION="${{ env.BACKUP_RETENTION_DAYS }}"
          RETENTION="${RETENTION:-30}"
          
          if ! gsutil ls "gs://${{ env.BACKUP_BUCKET }}" 2>/dev/null; then
            echo "âŒ Bucket gs://${{ env.BACKUP_BUCKET }} does not exist. Please create it manually with proper permissions."
            echo "Required permissions for the service account:"
            echo "- Storage Object Admin or Storage Admin role"
            echo "- Or specific permissions: storage.objects.create, storage.objects.delete, storage.objects.get, storage.objects.list"
            exit 1
          else
            echo "âœ… Bucket gs://${{ env.BACKUP_BUCKET }} exists"
          fi

      - name: Upload to Google Cloud Storage
        if: github.event.inputs.upload_to_gcs != 'false'
        run: |
          gsutil cp "${{ steps.backup.outputs.file }}" "gs://${{ env.BACKUP_BUCKET }}/"
          echo "âœ… Backup uploaded to gs://${{ env.BACKUP_BUCKET }}/${{ steps.backup.outputs.file }}"

      - name: List recent backups
        run: |
          echo "ðŸ“¦ Recent backups in GCS:"
          gsutil ls -l "gs://${{ env.BACKUP_BUCKET }}/" | tail -10

      - name: Cleanup old local backups
        run: |
          rm -f *.sql.gz
          echo "âœ… Local cleanup done"

      - name: Summary
        run: |
          RETENTION="${{ env.BACKUP_RETENTION_DAYS }}"
          RETENTION="${RETENTION:-30}"
          echo "## ðŸ’¾ Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp:** ${{ steps.backup.outputs.timestamp }}" >> $GITHUB_STEP_SUMMARY
          echo "- **File:** ${{ steps.backup.outputs.file }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Storage:** gs://${{ env.BACKUP_BUCKET }}/${{ steps.backup.outputs.file }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Retention:** ${RETENTION} days" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Backup completed successfully!" >> $GITHUB_STEP_SUMMARY
