name: Daily Neon Database Backup

on:
  schedule:
    # Tous les jours à 2h du matin (UTC)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Permet de lancer manuellement

env:
  BACKUP_BUCKET: ${{ vars.BACKUP_BUCKET || 'bikininjas-budget-app-backups' }}

jobs:
  backup:
    name: Backup Neon Database
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Google Auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCS_GH_SVC_ACCOUNT_JSON_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Get Database URL from Secret Manager
        id: get-db-url
        run: |
          DB_URL=$(gcloud secrets versions access latest --secret="${{ vars.DATABASE_URL_SECRET }}")
          echo "::add-mask::$DB_URL"
          echo "DATABASE_URL=$DB_URL" >> $GITHUB_ENV

      - name: Create PostgreSQL Backup
        run: |
          # Installer PostgreSQL client 17
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17
          
          # Créer le backup
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="neon_backup_${TIMESTAMP}.sql.gz"
          
          echo "Creating backup: $BACKUP_FILE"
          
          # Convertir l'URL SQLAlchemy (postgresql+asyncpg) en URL PostgreSQL standard
          PGDUMP_URL="${DATABASE_URL}"
          PGDUMP_URL="${PGDUMP_URL//postgresql+asyncpg:\/\//postgresql:\/\/}"
          
          # Remplacer les paramètres SSL invalides avec sed
          # Gérer tous les cas: ?ssl, &ssl=..., ?sslmode=..., etc.
          PGDUMP_URL=$(echo "$PGDUMP_URL" | sed -E 's/[?&]ssl([^m]|$)/?sslmode=require\1/g')
          
          echo "URL protocol: $(echo $PGDUMP_URL | cut -d':' -f1)"
          
          # Utiliser pg_dump avec compression directe
          pg_dump "${PGDUMP_URL}" | gzip > "${BACKUP_FILE}"
          
          # Vérifier que le backup a été créé
          if [ ! -f "${BACKUP_FILE}" ]; then
            echo "❌ Backup file not created"
            exit 1
          fi
          
          echo "✅ Backup created successfully ($(du -h ${BACKUP_FILE} | cut -f1))"
          echo "BACKUP_FILE=${BACKUP_FILE}" >> $GITHUB_ENV

      - name: Upload to Google Cloud Storage
        run: |
          BUCKET_PATH="gs://${{ env.BACKUP_BUCKET }}/neon/$(date +%Y/%m)/"
          
          echo "Uploading to: $BUCKET_PATH"
          gsutil cp "${BACKUP_FILE}" "${BUCKET_PATH}"
          
          echo "✅ Backup uploaded: ${BUCKET_PATH}${BACKUP_FILE}"

      - name: Cleanup Old Backups (Keep 30 days)
        run: |
          CUTOFF_DATE=$(date -d '30 days ago' +%Y-%m-%d)
          echo "Removing backups older than $CUTOFF_DATE"
          
          # Liste tous les backups et supprime ceux de plus de 30 jours
          gsutil ls -l "gs://${{ env.BACKUP_BUCKET }}/neon/**/*.sql.gz" | \
            awk '{if($2 < "'$CUTOFF_DATE'") print $3}' | \
            grep -v '^$' | \
            xargs -r -I {} gsutil rm {} || echo "No old backups to delete"
          
          echo "✅ Cleanup completed"
